Strictly speaking, as one's knowledge of the universe expands, some theorems will be disproved. Assuming that 10% of the information within the universe is known to man (actually much less than this), theorems are formed based on the available information. When man knows 80% of the universe, many theorems no longer hold.
A model is a simplification of a complex world. It is a better choice in the case of limited human arithmetic power and limited information. For example, if HR needs to recruit a hardworking and smart person to work, in his limited time (he can not go to work with each candidate for a few days to reach a conclusion), the most simple and efficient way is to filter by education. The model is: people with good education have a higher probability of being hardworking and smart.
But in reality, I know many smart people who are good at finding connections between things, defining problems from chaos, finding solutions, and promoting a framework to reduce chaos, but they don't have top university degrees.
The development of computers has alleviated the shortcomings of insufficient human arithmetic power, and the increasing amount of data that can be recorded and used in the information age has alleviated the shortcomings of insufficient information. It is predictable that man will increasingly rely on computer algorithms to replace his own calculations, and all man needs to do is clear what is important to him to adjust the parameters of his model.
Models need trade off between bias and variance. If HR use only attribute to predict a candidate's future performance, the bias is very high. If he use 10 attributes to predict a candidate, the bias would be much smaller. But if he use 1000 attributes(if he has enough time), the bias would be very low, but variance is high.
